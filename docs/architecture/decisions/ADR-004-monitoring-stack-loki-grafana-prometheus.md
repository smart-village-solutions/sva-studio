# ADR-004: Monitoring Stack ‚Äì Loki, Grafana & Prometheus

**Datum:** 5. Februar 2026
**Status:** ‚úÖ Accepted
**Kontext:** System Monitoring, Logging & Observability f√ºr SVA Studio CMS 2.0
**Entscheider:** SVA Studio Team

---

## Entscheidung

Wir implementieren einen integrierten Observability Stack aus:
- **Prometheus** f√ºr Metriken und System-KPIs
- **Loki** f√ºr strukturierte Logs
- **Grafana** als zentale Visualisierungs- und Alerting-Plattform

---

## Kontext und Problem

**Architektur-√úberblick:**
- **Betrieb durch Dienstleister:** Prometheus + Loki + Grafana laufen auf Dienstleister-Infrastruktur (zentral betrieben)
- **Kommunen erhalten vereinfachte Ansichten:** Im SVA Studio CMS gibt es Simple Dashboards/Logs, NICHT direkter Zugriff auf Grafana
- **Strikte Multi-Tenancy:** Kommunen k√∂nnen ihre Daten nicht gegenseitig sehen

Der Dienstleister ben√∂tigt umfassende √úberwachung f√ºr:
- **System Health:** CPU, RAM, Disk, Netzwerk, DB-Performance (alle Kommunen, zentral)
- **Application Logging:** Strukturierte JSON-Logs mit Kontextinformationen (User-ID, Request-ID, Session-ID, Workspace-ID)
- **Audit Trails:** Vollst√§ndige Protokollierung kritischer Aktionen (Append-Only, unver√§nderlich, 2 Jahre Retention)
- **Performance Monitoring:** Response-Zeiten, Error-Rates, Cache-Hit-Rates pro Workspace
- **Anomalie-Erkennung:** Automatische Baselines und Abweichungserkennung f√ºr Capacity Planning
- **Alert Management:** Rule-basierte Alerts f√ºr kritische Fehler (E-Mail an Support-Team)
- **Multi-Tenant Isolation:** Strikte Daten-Trennung zwischen Kommunen auf Label-Ebene
- **Compliance:** DSGVO-konform (Anonymisierung, Datenschutz, Right-to-Deletion), 90 Tage Standard / bis 2 Jahre konfigurierbar

**Technische Anforderungen (Dienstleister-Seite):**
- High-Volume Log Ingestion (Millionen Events/Tag √ºber alle Kommunen)
- Sub-Sekunden Query Performance f√ºr Echtzeit-Dashboards (Dienstleister, Kommunen mit Caching)
- Cost-Efficient Storage (Time-Series + Log Compression, zentrale Kostenoptimierung)
- Sichere Isolation: Prometheus/Loki nur f√ºr Dienstleister erreichbar (nicht √∂ffentlich)
- Native Support f√ºr Labels/Tags zur Datenorganisation (workspace_id, tenant_id, component, etc.)
- **Kein direkter Kommunen-Zugriff:** Grafana ist Dienstleister-Tool, Kommunen nutzen vereinfachte SVA Studio Endpoints

**Kommune-Sicht (im SVA Studio CMS):**
- REST-API f√ºr Logs/Metriken (vereinfachte Queries, keine PromQL/LogQL-Exposure)
- Vordefinierte Dashboards (System-Health, Error-√úbersicht, Nutzungsstatistiken)
- Einfache Visualisierungen (Charts, Tables, Alerts)
- Kein Zugriff auf andere Kommunen oder System-Metriken

---

## Betrachtete Optionen

| Stack | Logs | Metriken | Dashboards | Type Safety | Operator-Freundlich | Cost | Bewertung |
|---|---|---|---|---|---|---|---|
| **Prometheus + Loki + Grafana** | ‚úÖ Loki | ‚úÖ Prometheus | ‚úÖ Grafana | 9/10 | 9/10 | 10/10 | **9.3/10** ‚úÖ |
| ELK Stack (Elasticsearch) | ‚úÖ (teuer) | üü° Metriken-Plugin | ‚úÖ Kibana | 7/10 | 7/10 | 5/10 | 6.7/10 |
| Datadog | ‚úÖ All-in-One | ‚úÖ All-in-One | ‚úÖ All-in-One | 8/10 | 10/10 | 3/10 (SaaS) | 7/10 |
| Splunk | ‚úÖ (sehr teuer) | üü° Add-ons | ‚úÖ Dashboards | 6/10 | 6/10 | 2/10 | 4.7/10 |
| New Relic | ‚úÖ All-in-One | ‚úÖ All-in-One | ‚úÖ All-in-One | 7/10 | 9/10 | 4/10 (SaaS) | 6.7/10 |

### Warum Prometheus + Loki + Grafana?

#### **1. Perfekte Komplementarit√§t:**
```
Prometheus (Metriken)
‚îú‚îÄ‚îÄ Time-Series DB optimiert f√ºr numerische Daten
‚îú‚îÄ‚îÄ PromQL f√ºr flexible Queries und Aggregationen
‚îú‚îÄ‚îÄ Built-in Alerting Rules
‚îî‚îÄ‚îÄ Native Exporters f√ºr Standard-Services

Loki (Logs)
‚îú‚îÄ‚îÄ Log-Indexierung optimiert f√ºr Labels, nicht volltext
‚îú‚îÄ‚îÄ LogQL (Prometheus-√§hnlich) f√ºr konsistentes Querying
‚îú‚îÄ‚îÄ Hohe Compression (Log-Daten bis 10x komprimierbar)
‚îî‚îÄ‚îÄ Label-basierte Multi-Tenancy (Workspace/Kommune Isolation)

Grafana (Visualization & Alerting)
‚îú‚îÄ‚îÄ Unterst√ºtzt Prometheus, Loki, SQL, CloudWatch nativ
‚îú‚îÄ‚îÄ Unified Dashboards (Metriken + Logs zusammen)
‚îú‚îÄ‚îÄ Advanced Alerting (Multi-Channel: E-Mail, Slack, Webhook, SMS)
‚îî‚îÄ‚îÄ Plugin-√ñkosystem f√ºr Erweiterungen
```

#### **2. Cost-Efficiency:**
```
Datadog (SaaS):
‚îú‚îÄ‚îÄ Logs: $0.70 pro GB/Monat ‚Üí 100GB = $70/Monat
‚îú‚îÄ‚îÄ Metriken: $0.05 pro Metric/Monat ‚Üí 1000 Metrics = $50/Monat
‚îî‚îÄ‚îÄ Total: ~$120-200/Monat f√ºr Small-Mid Setup

ELK Stack (Self-Hosted):
‚îú‚îÄ‚îÄ Elasticsearch (Speicher-hungrig): 16GB RAM + 500GB Storage
‚îú‚îÄ‚îÄ Logstash (CPU-intensiv): 4+ CPU Cores
‚îú‚îÄ‚îÄ Kibana (einfach): 2GB RAM
‚îî‚îÄ‚îÄ Total: ~$400-600/Monat Infrastruktur-Kosten

Prometheus + Loki + Grafana (Self-Hosted):
‚îú‚îÄ‚îÄ Prometheus: 2GB RAM, 100GB Storage (komprimiert)
‚îú‚îÄ‚îÄ Loki: 2GB RAM, 200GB Storage (Indexes leicht)
‚îú‚îÄ‚îÄ Grafana: 512MB RAM, minimal Speicher
‚îî‚îÄ‚îÄ Total: ~$100-150/Monat Infrastruktur-Kosten (75% g√ºnstiger als ELK)
```

#### **3. Developer Experience:**
- **Konsistente Query-Sprachen:** PromQL (Metriken) und LogQL (Logs) √§hnliche Syntax
- **Native Integrations:** Winston/Pino f√ºr Logs, prom-client f√ºr Metriken
- **Unified Dashboards:** Metriken + Logs in einem Grafana-View (Korrelation)

#### **4. OpenTelemetry-Integration (Zukunftssicherheit):**

**Strategie:** Applikationen nutzen **OpenTelemetry SDK** statt direkter Prometheus/Loki-Clients.

**Begr√ºndung:**
- **Vendor-Neutralit√§t:** App-Code unabh√§ngig vom Backend (Prometheus/Loki/Datadog/...)
- **Standards-Compliance:** W3C Trace Context, OpenMetrics, OTLP Protocol
- **Migration-Sicherheit:** Backend-Wechsel = Config-√Ñnderung (2-3 Tage), nicht Code-Refactoring (3-4 Wochen)
- **Unified Observability:** Traces + Metrics + Logs mit einem SDK, automatische Korrelation
- **TanStack Start Kompatibilit√§t:** OTEL SDK funktioniert mit Server Functions und Middleware

**Praktischer Vorteil:**
```
Migration zu Datadog (falls n√∂tig):
‚îú‚îÄ‚îÄ Mit OTEL: 0 Code-√Ñnderungen, nur OTEL Collector Config swap
‚îú‚îÄ‚îÄ Ohne OTEL: 3-4 Wochen App-Code Refactoring, 2-3 Tage Downtime
‚îî‚îÄ‚îÄ Fazit: OTEL reduziert Lock-in-Risiko massiv
```

#### **5. Kubernetes-Native:**
- **Service Discovery:** Prometheus Operator + ServiceMonitors (automatische Target-Erkennung)
- **DaemonSet Deployment:** Promtail auf jedem K8s-Node (Log-Collection)
- **Helm Charts:** Offizielle Charts f√ºr Prometheus, Loki, Grafana (Infrastructure-as-Code)

#### **6. Multi-Tenant Isolation (Dienstleister-Betrieb):**
```
Prometheus Labels (Metriken)
‚îú‚îÄ‚îÄ workspace_id: "Kommune_A"      // MANDATORY Label
‚îú‚îÄ‚îÄ environment: "production"
‚îú‚îÄ‚îÄ service: "sva-studio-backend"
‚îî‚îÄ‚îÄ Nur Dienstleister sieht alle Metriken

Loki Labels (Logs)
‚îú‚îÄ‚îÄ workspace_id: "Kommune_A"      // MANDATORY, garantiert Isolation
‚îú‚îÄ‚îÄ component: "auth"
‚îú‚îÄ‚îÄ level: "error"
‚îî‚îÄ‚îÄ LogQL-Queries im Grafana: {workspace_id="Kommune_A"} filters strict by Dienstleister

Grafana (Dienstleister-Tool)
‚îú‚îÄ‚îÄ Multi-Workspace Dashboards (Dienstleister-View)
‚îú‚îÄ‚îÄ Alert Rules pro Workspace (z.B. hohe Error-Rate in Kommune_A)
‚îú‚îÄ‚îÄ Support-Team hat Zugriff auf alle Daten f√ºr Debugging
‚îî‚îÄ‚îÄ Rollenbasiert: Admin, Operator, Viewer (kein Zugriff auf andere Workspaces)

SVA Studio CMS (Kommune-View, vereinfacht)
‚îú‚îÄ‚îÄ REST-Endpoint: GET /api/admin/monitoring/dashboard
‚îú‚îÄ‚îÄ R√ºckgabe: {errorRate, avgResponseTime, activeUsers, lastErrors}
‚îú‚îÄ‚îÄ Keine Rohdaten, nur aggregierte Metriken
‚îú‚îÄ‚îÄ Streng isoliert: Queries intern mit workspace_id gefiltert
‚îî‚îÄ‚îÄ Rate-Limiting: 1 Request/Sekunde pro Kommune (DDoS-Schutz)
```

---

## Trade-offs & Limitierungen

### Pros
- ‚úÖ **Cost-Efficient:** ~75% g√ºnstiger als ELK oder Datadog (Self-Hosted)
- ‚úÖ **Open Source:** Keine Vendor Lock-in, vollst√§ndige Kontrolle
- ‚úÖ **Highly Scalable:** Prometheus verarbeitet Millionen Metrics/Min, Loki Billionen Logs/Tag
- ‚úÖ **Unified Platform:** Ein Dashboard f√ºr Logs + Metriken + Alerting
- ‚úÖ **Developer-Friendly:** PromQL + LogQL sind konsistent, einfach zu lernen
- ‚úÖ **Kubernetes-Native:** Service Discovery, Helm Charts, Prometheus Operator
- ‚úÖ **DSGVO-Konform:** Self-Hosted, vollst√§ndige Datenkontrolle, einfache Anonymisierung
- ‚úÖ **Rich Ecosystem:** 1000+ Exporters, Grafana Plugins, Community Support

### Cons
- ‚ùå **Dienstleister-Maintenance:** Require Operations Knowledge (h√∂here Anforderung an Support-Team)
- ‚ùå **Storage Management:** Retention-Policies m√ºssen zentral gepflegt werden (Multi-Tenant Komplexit√§t)
- ‚ùå **Cardinality Issues:** Hohe Label-Cardinality √ºber viele Kommunen ‚áí Storage-Explosion (z.B. User-IDs als Labels)
- ‚ùå **Full-Text Search Limitation:** LogQL optimiert f√ºr Labels, nicht f√ºr komplexe Log-Suche
- ‚ùå **Beta Features:** Some Loki features (wie Patterns) sind noch nicht stabilisiert
- ‚ùå **HA Komplexit√§t:** Single Prometheus/Loki = Single Point of Failure (backup strategy n√∂tig)

### Mitigationen
```
‚ùå Dienstleister-Maintenance
‚îî‚îÄ Mitigation: Infrastructure-as-Code (Terraform/Helm), dokumentierte Runbooks, SLA f√ºr Response-Zeit

‚ùå Storage Management (Multi-Tenant)
‚îî‚îÄ Mitigation: Loki Table Manager f√ºr automatische Retention pro Workspace, Quotas pro Kommune
   ‚îî‚îÄ z.B. Kommune_A: max 500GB Logs, 90 Tage Retention

‚ùå Cardinality Issues
‚îî‚îÄ Mitigation: Strict Label Guidelines (keine User-IDs/unique values als Labels)
   ‚îú‚îÄ ALLOWED: workspace_id, component, level, status_code
   ‚îî‚îÄ FORBIDDEN: user_id, session_id, email
   ‚îî‚îÄ Cardinality Monitoring: Alert bei > 100k unique metric combinations

‚ùå Full-Text Search
‚îî‚îÄ Mitigation: F√ºr Audit-Logs Elasticsearch als optionales Backup
   ‚îî‚îÄ Loki = primary (schnell, kosteneffizient), ES = archival (Compliance)

‚ùå HA/Disaster Recovery
‚îî‚îÄ Mitigation:
   ‚îú‚îÄ Prometheus + Loki: 2 replicas behind Load Balancer
   ‚îú‚îÄ Persistent Volume mit Backup (daily, 7 Tage retention)
   ‚îú‚îÄ RPO (Recovery Point Objective): 1 Hour
   ‚îú‚îÄ RTO (Recovery Time Objective): 15 Minuten
   ‚îî‚îÄ Test DR monatlich
```

---

## Sicherheit & Compliance

### Technische Komponenten
- **Prometheus:** Metrics Collection + AlertManager
- **Loki:** Log Aggregation + Promtail (Collector)
- **Grafana:** Visualisierung + LDAP/SAML Auth
- **External Monitoring:** UptimeRobot (SPOF-√úberwachung)

### Multi-Tenancy & Isolation
- **Label-Enforcement:** `workspace_id` MANDATORY f√ºr alle Logs/Metriken (sonst Rejection)
- **Zugriffskontrolle:** Prometheus/Loki nur Dienstleister-intern, Kommunen via REST-API
- **Rate-Limiting:** 1000 logs/sec pro Workspace, 100 API-Requests/min

### DSGVO-Compliance
- **Anonymisierung:** IP-Adressen (letztes Oktett), User-Agent normalisiert
- **Retention:** 90 Tage Standard, 2 Jahre Audit-Logs
- **Right-to-Erasure:** Automatische L√∂schung auf Anfrage (30 Tage Best√§tigung)

## Rahmenbedingungen f√ºr Umsetzung

### Label-Schema (Pflicht)
**Erlaubte Labels** (Low Cardinality): `workspace_id`, `component`, `environment`, `status_code`, `level`
**Verbotene Labels** (High Cardinality): `user_id`, `session_id`, `email`, `request_id`

### Cardinality-Limits
- Prometheus: Max 50k metric combinations pro Workspace
- Loki: Max 1k label combinations pro Stream
- Enforcement: OTEL SDK Whitelisting + Relabeling Rules

### Verantwortlichkeiten
**Dienstleister:** Stack-Betrieb (SLA 99.5%), Alert Response (< 15 Min), Backup-Tests
**Kommunen:** Dashboard-Zugriff via CMS, Support via Ticket-System

---

## Skalierung & Capacity Planning

### Auslegung (150 Kommunen)
- **Prometheus:** 1.125M Metrics, 500 IOPS SSD, Cardinality-Monitoring
- **Loki:** 5000 logs/sec (HA: 10k logs/sec), 30GB/day Storage (komprimiert)
- **Grafana:** 50 concurrent users, 500+ Dashboards

### Scale-Trigger
- **Scale UP:** Cardinality > 80%, Loki ingestion > 7000 logs/sec, Query Latency > 2sec
- **Scale OUT:** Latency p99 > 500ms (regionale Instanzen)

---

## Alerting & Verf√ºgbarkeit

### Alert-Kategorien
- **Critical (< 5 Min):** Prometheus/Loki/Grafana Down, Cardinality-Explosion
- **Warning (< 15 Min):** Disk > 85%, Query Latency > 2sec, Backup Failed
- **Info:** Retention Cleanup, neue Labels erkannt

### External Monitoring
- **Tool:** UptimeRobot (oder Pingdom)
- **Zweck:** SPOF-√úberwachung (wenn interne Alerts ausfallen)
- **Checks:** Health-Endpoints f√ºr Prometheus/Loki/Grafana alle 5 Min
- **Action:** Email + PagerDuty bei Ausfall

## High Availability

### HA-Setup
- **Prometheus:** 2 Replicas (active-active), S3 Snapshots, RPO 10min, RTO 15-30min
- **Loki:** 2 Replicas (stateless via S3), Redis Cache, RPO 0, RTO < 5min
- **AlertManager:** Gossip Cluster (2 Instances), 0 Alert Loss

### Backup-Strategie
- **Daily S3 Snapshots** (Prometheus TSDB)
- **S3 Lifecycle Policies** (Loki Retention)
- **Monatliche Restore-Tests**

---

## Implementierung / Roadmap

### Phase 1: Foundation (Woche 1-4, nicht 1-3)
- [ ] Prometheus Setup (HA with 2 replicas, Persistent Volumes, AlertManager)
- [ ] Loki Setup (HA 2 replicas, S3 backend storage, Redis cache)
- [ ] Grafana Setup + LDAP/SAML Integration + HA setup
- [ ] Firewall-Konfiguration (nur Dienstleister-Access)
- [ ] Backup-Strategie (daily S3 snapshots, 7 Tage retention)
- [ ] Disaster Recovery Test (restore from backup, record RTO)
- [ ] **UptimeRobot Setup**: Health-Check Endpoints, Email + Webhook alerts
- [ ] **AlertManager Config**: Critical/Warning/Info rules defined

### Phase 2: Application Integration (Woche 5-7, nicht 4-6)
- [ ] Structured Logging (Winston/Pino mit workspace_id Injection)
- [ ] Custom Metrics (API Response Times, Business Events)
- [ ] Audit Log Pipeline (Application ‚Üí Loki mit Append-Only)
- [ ] Error Tracking (Errors ‚Üí Grafana Alerts ‚Üí Dienstleister-Email)
- [ ] Retention Policies (90 Tage default, konfigurierbar pro Workspace)
- [ ] **Label Validation**: workspace_id MANDATORY, return 400 if missing
- [ ] **Promtail Deployment**: DaemonSet per K8s Node, local buffering

### Phase 3: Kommune-Integration (Woche 8-9, nicht 7-8)
- [ ] REST-API f√ºr Logs/Metrics (vereinfachte Queries, workspace_id enforcement)
- [ ] SVA Studio CMS: Dashboard Integration
- [ ] Vordefinierte Charts (Error-Rate, Response-Time, User-Count)
- [ ] Alert-Notification im CMS (kritische Events)
- [ ] Rate-Limiting & Security Headers (100 req/min per workspace)
- [ ] **Data Export API**: GET /api/admin/monitoring/export (JSON, CSV)

### Phase 4: DSGVO & Testing & Production Hardening (Woche 10-13, nicht 9-10)
- [ ] Anonymization Filters (IP, User-Agent, PII)
- [ ] Right-to-Erasure Implementation & Testing
- [ ] DSGVO Compliance-Report Generator (monthly)
- [ ] Load Test: 150M logs/day, 5000 logs/sec sustained, verify limits
- [ ] HA Failover Test (each component individually + combined)
- [ ] **Cardinality Load Test**: 150 Workspaces √ó 7500 Metrics = 1.125M metrics
- [ ] **UptimeRobot Response Time**: Verify < 5sec health-checks
- [ ] Stress Test Alerting: 1000 alerts/min, verify no loss
- [ ] Backup & Restore Drill: Restore from S3, verify data integrity

### Validierung & SLOs
- [ ] Verf√ºgbarkeit: 99.5% (Dienstleister-Stack), 99% (Kommune-API)
- [ ] Query Performance: Grafana Dashboards < 2 Sekunden, CMS-API < 1 Sekunde
- [ ] Alert Response: Kritische Alerts < 60 Sekunden, Email Versand < 120 Sekunden
- [ ] Data Loss: RPO 1 Hour, RTO 15 Minuten
- [ ] DSGVO Audit: Anonymisierung, Retention, L√∂schung funktionieren
- [ ] Cardinality Monitoring: Kein unkontrolliertes Wachstum

---

## Exit-Strategie

### Migration-Optionen
**Zu ELK Stack (3 Wochen):**
- **Trigger:** Cardinality > 1.5M oder Full-Text Search ben√∂tigt
- **Vorteil OTEL:** Nur OTEL Collector Config √§ndern, kein App-Code Refactoring

**Zu Datadog (2 Wochen):**
- **Trigger:** Ops-Aufwand > 20h/Monat, Budget verf√ºgbar
- **Vorteil OTEL:** 0 Code-√Ñnderungen, Parallel-Betrieb m√∂glich

### Review-Zeitplan
- **6 Monate:** Kosten, Ops-Aufwand, Cardinality evaluieren
- **12 Monate:** Stack-Fitness Review

**Nicht zu migrierende Komponente:**
- SVA Studio CMS API bleibt gleich (kommunen-facing API ist abstrahiert)
- Nur Dienstleister-interne Stack austauschbar

---

## Validierung & SLOs (finalisiert)
- [ ] Verf√ºgbarkeit: 99.5% (Dienstleister-Stack), 99% (Kommune-API)
- [ ] Query Performance: Grafana Dashboards < 2 Sekunden, CMS-API < 1 Sekunde
- [ ] Alert Response: Kritische Alerts < 60 Sekunden, Email Versand < 120 Sekunden
- [ ] Data Loss: RPO 1 Hour (Prometheus), RPO 0 (Loki via S3), RTO 15 Minuten
- [ ] External Monitoring: UptimeRobot alerts within 5 minutes of outage
- [ ] DSGVO Audit: Anonymisierung, Retention, L√∂schung funktionieren
- [ ] Cardinality Limits: Max 1M metrics (150 Workspaces √ó 7.5k metrics), alert at 80%
- [ ] Label Validation: 100% of logs have workspace_id, < 0.1% validation errors
- [ ] HA Test Results: All components recover < 15 min after single/double failures

---

**Links:**
- [Monitoring.md Requirements](../../../concepts/konzeption-cms-v2/02_Anforderungen/02_01_Funktional/Monitoring.md)
- [Prometheus Documentation](https://prometheus.io/docs/)
- [Loki Documentation](https://grafana.com/docs/loki/latest/)
- [Grafana Documentation](https://grafana.com/docs/grafana/)
- [ADR-001: Frontend Framework Selection](ADR-001-frontend-framework-selection.md)
